{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from pandarallel import pandarallel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pandarallel.initialize(progress_bar=True,\n",
    "                        nb_workers=6,\n",
    "                        #verbose=1\n",
    "                       )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/maurelco/Developer/Python/Projet 4/QueryResults-3.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Delete HTML"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['Body']= df['Body'].apply(lambda x: BeautifulSoup(x).get_text())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "type(df['Body'][1000])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['Body'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Build Raw corpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_corpus = \"\".join(df.Body)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_corpus[:10000]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_corpus = raw_corpus[:10000]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Text preprocessing function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def display_tokens_info(tokens):\n",
    "    \"\"\"display info about corpus\"\"\"\n",
    "    print(f\"nb tokens {len(tokens)}, nb tokens uniques {len(set(tokens))}\")\n",
    "    print(tokens[:30])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_un(tokens):\n",
    "    unrelevant_tags =  ['CD','IN','CC','RB']\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    for i in range(len(tags)):\n",
    "        good_words = [word for word,pos in tags if (pos not in unrelevant_tags)]\n",
    "    return good_words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_text(doc,\n",
    "                   rejoin=False,\n",
    "                   list_rare_words=None,\n",
    "                    min_len_word=1,\n",
    "                    force_is_alpha=False,\n",
    "                    eng_words = False\n",
    "                 ) :\n",
    "    \"\"\"function of text processing\n",
    "\n",
    "    positional arguments:\n",
    "    ----------------------\n",
    "    doc: str : the document (aka a text in str format) to process\n",
    "\n",
    "    opt arguments:\n",
    "    ---------------------\n",
    "    rejoin: bool : if True return a string else return a list of tokens\n",
    "    list_rare_words: list : a list of rare words to exclude\n",
    "    min_len_words: int : the minimum lenght of words to not exclude\n",
    "    force_is_alpha: int : if 1, exclude all tokens with a numeric character\n",
    "\n",
    "    return:\n",
    "    --------------------\n",
    "    a string (if rejoin is True) or a list of tokens\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #lower\n",
    "    print(type(doc))\n",
    "    doc = doc.lower()\n",
    "    print(type(doc))\n",
    "\n",
    "    #tokenizer\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "\n",
    "    # stop words:\n",
    "    english_stop_words=nltk.corpus.stopwords.words('English')\n",
    "    cleaned_tokens_list = [word for word in raw_tokens_list if word not in english_stop_words]\n",
    "\n",
    "    #############################################################################################\n",
    "\n",
    "    # no rare tokens\n",
    "    # non_rare_tokens = [w for w in cleaned_tokens_list if w not in list_rare_words]\n",
    "\n",
    "    # no more len words\n",
    "    more_than_N = [w for w in cleaned_tokens_list if len(w) >= min_len_word]\n",
    "\n",
    "    # only alpha characters\n",
    "    if force_is_alpha:\n",
    "        alpha_tokens = [w for w in more_than_N if w.isalpha()]\n",
    "    else:\n",
    "        alpha_tokens = more_than_N\n",
    "\n",
    "    #############################################################################################\n",
    "\n",
    "    #lemmatizer\n",
    "    trans = WordNetLemmatizer()\n",
    "    trans_text = [trans.lemmatize(word) for word in alpha_tokens]\n",
    "\n",
    "\n",
    "    #############################################################################################\n",
    "\n",
    "    good_words = remove_un(engl_text)\n",
    "\n",
    "    if rejoin:\n",
    "        return \" \".join(good_words)\n",
    "\n",
    "    return good_words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def final_clean(doc):\n",
    "    new_doc = process_text(doc,\n",
    "                           rejoin=False,\n",
    "                           list_rare_words=None,\n",
    "                           min_len_word=1,\n",
    "                           force_is_alpha=True,\n",
    "                           eng_words = False\n",
    "                           )\n",
    "    return new_doc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['Body']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['Body'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['_clean_text']= df['Body'].parallel_apply(lambda x : final_clean(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['Body']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "type(raw_corpus[])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_corpus[3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corpus = process_text(raw_corpus)\n",
    "display_tokens_info(corpus)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tmp= pd.Serie(corpus).value_counts()\n",
    "tmp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "array_test = []`\n",
    "df['Body'].apply(lambda x: nltk.FreqDist(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " for k, v in corpora.iteritems():\n",
    "    freq[k] = fq = nltk.FreqDist(v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Premièrement, on récupère la fréquence totale de chaque mot sur tout le corpus d'artistes\n",
    "freq_totale = nltk.Counter()\n",
    "for k, v in corpora.iteritems():\n",
    "    freq_totale += freq[k]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "most_freq = zip(*freq2.most_common(100))[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "  for k, v in corpora.iteritems():\n",
    "        freq[k] = fq = nltk.FreqDist(v)\n",
    "        stats[k] = {'total': len(v), 'unique': len(fq.keys())}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}